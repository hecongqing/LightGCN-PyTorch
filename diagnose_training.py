"""
è¯Šæ–­é˜¿é‡Œå·´å·´æ•°æ®é›†è®­ç»ƒlossä¸åŠ¨çš„é—®é¢˜
æ£€æŸ¥æ•°æ®é¢„å¤„ç†ã€æ¨¡å‹è®¾ç½®ã€è®­ç»ƒè¿‡ç¨‹ç­‰å„ä¸ªç¯èŠ‚
"""

import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

def create_test_data():
    """åˆ›å»ºæµ‹è¯•æ•°æ®æ¥è¯Šæ–­é—®é¢˜"""
    print("=== åˆ›å»ºæµ‹è¯•æ•°æ® ===")
    
    # åˆ›å»ºæ¨¡æ‹Ÿçš„é˜¿é‡Œå·´å·´æ•°æ®æ ¼å¼
    np.random.seed(42)
    
    num_users_raw = 2000
    num_items_raw = 1000
    num_interactions = 15000
    
    # ç”Ÿæˆç”¨æˆ·è¡Œä¸ºæ•°æ®
    user_data = pd.DataFrame({
        'user_id': np.random.randint(1, num_users_raw + 1, num_interactions),
        'item_id': np.random.randint(1, num_items_raw + 1, num_interactions),
        'behavior_type': np.random.choice([1, 2, 3, 4], num_interactions, p=[0.5, 0.2, 0.2, 0.1]),
        'time': pd.date_range('2014-11-18', '2014-12-19', periods=num_interactions)
    })
    
    # ç”Ÿæˆå•†å“æ•°æ®ï¼ˆåªä¿ç•™éƒ¨åˆ†å•†å“ï¼‰
    selected_items = np.random.choice(range(1, num_items_raw + 1), size=500, replace=False)
    item_data = pd.DataFrame({
        'item_id': selected_items
    })
    
    return user_data, item_data

def analyze_data_preprocessing(user_data, item_data, behavior_types=[4]):
    """åˆ†ææ•°æ®é¢„å¤„ç†æ­¥éª¤"""
    print("\n=== æ•°æ®é¢„å¤„ç†åˆ†æ ===")
    
    print(f"åŸå§‹ç”¨æˆ·è¡Œä¸ºæ•°æ®: {user_data.shape}")
    print(f"åŸå§‹å•†å“æ•°æ®: {item_data.shape}")
    print(f"åŸå§‹è¡Œä¸ºç±»å‹åˆ†å¸ƒ:")
    print(user_data['behavior_type'].value_counts().sort_index())
    
    # 1. è¿‡æ»¤æŒ‡å®šè¡Œä¸ºç±»å‹
    user_data_filtered = user_data[user_data['behavior_type'].isin(behavior_types)]
    print(f"\nè¿‡æ»¤åæ•°æ®: {user_data_filtered.shape}")
    
    # 2. åªä¿ç•™å•†å“å­é›†ä¸­çš„å•†å“
    item_ids = set(item_data['item_id'].unique())
    user_data_filtered = user_data_filtered[user_data_filtered['item_id'].isin(item_ids)]
    print(f"å•†å“è¿‡æ»¤åæ•°æ®: {user_data_filtered.shape}")
    
    # 3. æ£€æŸ¥ç”¨æˆ·å’Œå•†å“çš„äº¤äº’é¢‘æ¬¡åˆ†å¸ƒ
    user_inter_count = user_data_filtered.groupby('user_id').size()
    item_inter_count = user_data_filtered.groupby('item_id').size()
    
    print(f"\nç”¨æˆ·äº¤äº’é¢‘æ¬¡ç»Ÿè®¡:")
    print(f"æœ€å°å€¼: {user_inter_count.min()}, æœ€å¤§å€¼: {user_inter_count.max()}, å¹³å‡å€¼: {user_inter_count.mean():.2f}")
    print(f"å°‘äº5æ¬¡äº¤äº’çš„ç”¨æˆ·: {(user_inter_count < 5).sum()} / {len(user_inter_count)} ({(user_inter_count < 5).mean()*100:.1f}%)")
    
    print(f"\nå•†å“äº¤äº’é¢‘æ¬¡ç»Ÿè®¡:")
    print(f"æœ€å°å€¼: {item_inter_count.min()}, æœ€å¤§å€¼: {item_inter_count.max()}, å¹³å‡å€¼: {item_inter_count.mean():.2f}")
    print(f"å°‘äº5æ¬¡äº¤äº’çš„å•†å“: {(item_inter_count < 5).sum()} / {len(item_inter_count)} ({(item_inter_count < 5).mean()*100:.1f}%)")
    
    # 4. åº”ç”¨æœ€å°äº¤äº’é¢‘æ¬¡è¿‡æ»¤ (æ‚¨çš„ä»£ç ä¸­çš„æ­¥éª¤)
    N = 5
    valid_users = set(user_inter_count[user_inter_count >= N].index)
    valid_items = set(item_inter_count[item_inter_count >= N].index)
    
    user_data_final = user_data_filtered[
        user_data_filtered['user_id'].isin(valid_users) &
        user_data_filtered['item_id'].isin(valid_items)
    ]
    
    print(f"\næœ€ç»ˆæ•°æ® (æœ€å°‘{N}æ¬¡äº¤äº’): {user_data_final.shape}")
    
    if len(user_data_final) == 0:
        print("âŒ è­¦å‘Š: è¿‡æ»¤åæ•°æ®ä¸ºç©ºï¼è¿™ä¼šå¯¼è‡´è®­ç»ƒæ— æ³•è¿›è¡Œã€‚")
        return None
    
    # 5. ç¼–ç ç”¨æˆ·å’Œå•†å“ID
    user_encoder = LabelEncoder()
    item_encoder = LabelEncoder()
    
    user_data_final['user_id_encoded'] = user_encoder.fit_transform(user_data_final['user_id'])
    user_data_final['item_id_encoded'] = item_encoder.fit_transform(user_data_final['item_id'])
    
    num_users = user_data_final['user_id_encoded'].nunique()
    num_items = user_data_final['item_id_encoded'].nunique()
    num_interactions = len(user_data_final)
    
    print(f"\næœ€ç»ˆç»Ÿè®¡:")
    print(f"ç”¨æˆ·æ•°é‡: {num_users}")
    print(f"å•†å“æ•°é‡: {num_items}")
    print(f"äº¤äº’æ•°é‡: {num_interactions}")
    print(f"ç¨€ç–åº¦: {(1 - num_interactions / (num_users * num_items)) * 100:.2f}%")
    
    # æ£€æŸ¥æ•°æ®è´¨é‡
    if num_interactions < 1000:
        print("âŒ è­¦å‘Š: äº¤äº’æ•°é‡è¿‡å°‘ï¼Œå¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®š")
    if num_users < 100 or num_items < 100:
        print("âŒ è­¦å‘Š: ç”¨æˆ·æˆ–å•†å“æ•°é‡è¿‡å°‘ï¼Œå¯èƒ½å¯¼è‡´æ¨¡å‹å®¹é‡ä¸è¶³")
    
    return user_data_final, num_users, num_items

def check_model_and_loss():
    """æ£€æŸ¥æ¨¡å‹å’ŒæŸå¤±å‡½æ•°"""
    print("\n=== æ¨¡å‹å’ŒæŸå¤±æ£€æŸ¥ ===")
    
    # åˆ›å»ºå°è§„æ¨¡æµ‹è¯•æ¨¡å‹
    num_users, num_items = 100, 50
    embedding_dim = 16
    
    # ç®€åŒ–çš„æµ‹è¯•æ¨¡å‹
    class SimpleLightGCN(nn.Module):
        def __init__(self, num_users, num_items, embedding_dim):
            super().__init__()
            self.num_users = num_users
            self.num_items = num_items
            self.embedding = nn.Embedding(num_users + num_items, embedding_dim)
            nn.init.xavier_uniform_(self.embedding.weight)
            
        def forward(self, edge_index):
            x = self.embedding.weight
            return x[:self.num_users], x[self.num_users:]
            
        def predict(self, user_ids, item_ids, edge_index):
            user_emb, item_emb = self.forward(edge_index)
            user_emb = user_emb[user_ids]
            item_emb = item_emb[item_ids]
            return torch.sum(user_emb * item_emb, dim=1)
    
    model = SimpleLightGCN(num_users, num_items, embedding_dim)
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    batch_size = 32
    user_ids = torch.randint(0, num_users, (batch_size,))
    pos_item_ids = torch.randint(0, num_items, (batch_size,))
    neg_item_ids = torch.randint(0, num_items, (batch_size,))
    
    # åˆ›å»ºè™šæ‹Ÿè¾¹ç´¢å¼•
    edge_index = torch.tensor([[0, 1, 2], [num_users, num_users+1, num_users+2]], dtype=torch.long)
    
    # è®¡ç®—é¢„æµ‹å¾—åˆ†
    pos_scores = model.predict(user_ids, pos_item_ids, edge_index)
    neg_scores = model.predict(user_ids, neg_item_ids, edge_index)
    
    print(f"æ­£æ ·æœ¬å¾—åˆ†ç»Ÿè®¡: min={pos_scores.min():.4f}, max={pos_scores.max():.4f}, mean={pos_scores.mean():.4f}")
    print(f"è´Ÿæ ·æœ¬å¾—åˆ†ç»Ÿè®¡: min={neg_scores.min():.4f}, max={neg_scores.max():.4f}, mean={neg_scores.mean():.4f}")
    print(f"å¾—åˆ†å·®å¼‚: {(pos_scores - neg_scores).mean():.4f}")
    
    # æµ‹è¯•BPRæŸå¤±
    class SimpleBPRLoss(nn.Module):
        def __init__(self, lambda_reg=1e-4):
            super().__init__()
            self.lambda_reg = lambda_reg
            
        def forward(self, pos_scores, neg_scores, model):
            # BPRæŸå¤±
            bpr_loss = -torch.log(torch.sigmoid(pos_scores - neg_scores) + 1e-8).mean()
            
            # L2æ­£åˆ™åŒ–
            reg_loss = 0
            for param in model.parameters():
                reg_loss += torch.norm(param, p=2) ** 2
            reg_loss = self.lambda_reg * reg_loss
            
            return bpr_loss + reg_loss, bpr_loss.item(), reg_loss.item()
    
    criterion = SimpleBPRLoss()
    total_loss, bpr_loss, reg_loss = criterion(pos_scores, neg_scores, model)
    
    print(f"\næŸå¤±åˆ†æ:")
    print(f"BPRæŸå¤±: {bpr_loss:.6f}")
    print(f"æ­£åˆ™åŒ–æŸå¤±: {reg_loss:.6f}")
    print(f"æ€»æŸå¤±: {total_loss.item():.6f}")
    
    # æ£€æŸ¥æ¢¯åº¦
    total_loss.backward()
    grad_norms = []
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.norm().item()
            grad_norms.append(grad_norm)
            print(f"{name} æ¢¯åº¦èŒƒæ•°: {grad_norm:.6f}")
    
    if all(g < 1e-6 for g in grad_norms):
        print("âŒ è­¦å‘Š: æ¢¯åº¦è¿‡å°ï¼Œå¯èƒ½å¯¼è‡´å­¦ä¹ åœæ»")
    
    return model

def simulate_training_process():
    """æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼Œæ£€æŸ¥å¸¸è§é—®é¢˜"""
    print("\n=== è®­ç»ƒè¿‡ç¨‹æ¨¡æ‹Ÿ ===")
    
    # ä½¿ç”¨ä¹‹å‰çš„æµ‹è¯•æ•°æ®
    user_data, item_data = create_test_data()
    processed_data = analyze_data_preprocessing(user_data, item_data)
    
    if processed_data is None:
        print("âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒæ¨¡æ‹Ÿ")
        return
    
    user_data_final, num_users, num_items = processed_data
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®
    interactions = user_data_final[['user_id_encoded', 'item_id_encoded']].values
    print(f"è®­ç»ƒäº¤äº’æ•°: {len(interactions)}")
    
    # æ¨¡æ‹Ÿè´Ÿé‡‡æ ·
    def negative_sampling(user_id, user_item_dict, num_items, num_negatives=1):
        positive_items = user_item_dict.get(user_id, set())
        negative_items = []
        max_attempts = num_negatives * 10
        attempts = 0
        
        while len(negative_items) < num_negatives and attempts < max_attempts:
            item_id = np.random.randint(0, num_items)
            if item_id not in positive_items:
                negative_items.append(item_id)
            attempts += 1
            
        return negative_items if len(negative_items) == num_negatives else [0] * num_negatives
    
    # æ„å»ºç”¨æˆ·-å•†å“å­—å…¸
    user_item_dict = {}
    for user_id, item_id in interactions:
        if user_id not in user_item_dict:
            user_item_dict[user_id] = set()
        user_item_dict[user_id].add(item_id)
    
    # æ£€æŸ¥è´Ÿé‡‡æ ·è´¨é‡
    test_user = list(user_item_dict.keys())[0]
    neg_samples = negative_sampling(test_user, user_item_dict, num_items, 10)
    print(f"æµ‹è¯•ç”¨æˆ· {test_user} çš„æ­£æ ·æœ¬æ•°: {len(user_item_dict[test_user])}")
    print(f"è´Ÿé‡‡æ ·ç»“æœ: {neg_samples}")
    
    # æ¨¡æ‹Ÿä¸€ä¸ªè®­ç»ƒæ‰¹æ¬¡
    batch_size = min(64, len(interactions))
    batch_indices = np.random.choice(len(interactions), batch_size, replace=False)
    
    batch_users = []
    batch_pos_items = []
    batch_neg_items = []
    
    for idx in batch_indices:
        user_id, pos_item_id = interactions[idx]
        neg_item_id = negative_sampling(user_id, user_item_dict, num_items, 1)[0]
        
        batch_users.append(user_id)
        batch_pos_items.append(pos_item_id)
        batch_neg_items.append(neg_item_id)
    
    print(f"\næ‰¹æ¬¡ç»Ÿè®¡:")
    print(f"æ‰¹æ¬¡å¤§å°: {len(batch_users)}")
    print(f"ç”¨æˆ·IDèŒƒå›´: {min(batch_users)} - {max(batch_users)}")
    print(f"æ­£æ ·æœ¬å•†å“IDèŒƒå›´: {min(batch_pos_items)} - {max(batch_pos_items)}")
    print(f"è´Ÿæ ·æœ¬å•†å“IDèŒƒå›´: {min(batch_neg_items)} - {max(batch_neg_items)}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„æ­£è´Ÿæ ·æœ¬å¯¹
    pos_neg_pairs = set(zip(batch_users, batch_pos_items, batch_neg_items))
    if len(pos_neg_pairs) < len(batch_users):
        print("âŒ è­¦å‘Š: å‘ç°é‡å¤çš„è®­ç»ƒæ ·æœ¬")

def diagnose_loss_issues():
    """è¯Šæ–­lossä¸åŠ¨çš„å¸¸è§åŸå› """
    print("\n=== Lossé—®é¢˜è¯Šæ–­ ===")
    
    common_issues = [
        "1. å­¦ä¹ ç‡è®¾ç½®é—®é¢˜",
        "   - å­¦ä¹ ç‡è¿‡å°: lossä¸‹é™ææ…¢",
        "   - å­¦ä¹ ç‡è¿‡å¤§: losséœ‡è¡æˆ–å‘æ•£",
        "   - å»ºè®®: å°è¯• 1e-3, 1e-4, 1e-2",
        "",
        "2. æ•°æ®è´¨é‡é—®é¢˜", 
        "   - æ•°æ®è¿‡æ»¤è¿‡äºä¸¥æ ¼å¯¼è‡´æ ·æœ¬ä¸è¶³",
        "   - è´Ÿé‡‡æ ·è´¨é‡å·®",
        "   - æ•°æ®ç¨€ç–åº¦è¿‡é«˜",
        "",
        "3. æ¨¡å‹å®¹é‡é—®é¢˜",
        "   - åµŒå…¥ç»´åº¦è¿‡å°",
        "   - æ¨¡å‹å±‚æ•°ä¸åˆé€‚",
        "",
        "4. ä¼˜åŒ–å™¨é—®é¢˜",
        "   - æƒé‡è¡°å‡è¿‡å¤§",
        "   - æ‰¹æ¬¡å¤§å°ä¸åˆé€‚",
        "",
        "5. æŸå¤±å‡½æ•°é—®é¢˜",
        "   - BPRæŸå¤±è®¡ç®—æœ‰è¯¯",
        "   - æ­£åˆ™åŒ–ç³»æ•°è¿‡å¤§",
        "",
        "6. æ¢¯åº¦é—®é¢˜",
        "   - æ¢¯åº¦æ¶ˆå¤±",
        "   - æ¢¯åº¦çˆ†ç‚¸"
    ]
    
    for issue in common_issues:
        print(issue)

def main():
    """ä¸»è¯Šæ–­å‡½æ•°"""
    print("ğŸ” å¼€å§‹è¯Šæ–­é˜¿é‡Œå·´å·´æ•°æ®é›†è®­ç»ƒlossä¸åŠ¨é—®é¢˜...")
    
    # 1. åˆ†ææ•°æ®é¢„å¤„ç†
    user_data, item_data = create_test_data()
    processed_data = analyze_data_preprocessing(user_data, item_data)
    
    # 2. æ£€æŸ¥æ¨¡å‹å’ŒæŸå¤±
    if processed_data is not None:
        check_model_and_loss()
    
    # 3. æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    simulate_training_process()
    
    # 4. æä¾›è¯Šæ–­å»ºè®®
    diagnose_loss_issues()
    
    print("\n=== è§£å†³å»ºè®® ===")
    print("1. æ£€æŸ¥æ•°æ®é¢„å¤„ç†: ç¡®ä¿è¿‡æ»¤åä»æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ ·æœ¬")
    print("2. è°ƒæ•´å­¦ä¹ ç‡: å°è¯• 0.01, 0.001, 0.0001")  
    print("3. å‡å°‘æ­£åˆ™åŒ–: å°†weight_decayä»1e-4æ”¹ä¸º1e-5æˆ–1e-6")
    print("4. å¢åŠ åµŒå…¥ç»´åº¦: ä»64å¢åŠ åˆ°128æˆ–256")
    print("5. æ£€æŸ¥è´Ÿé‡‡æ ·: ç¡®ä¿è´Ÿæ ·æœ¬è´¨é‡")
    print("6. å‡å°‘è¿‡æ»¤æ¡ä»¶: å°†æœ€å°äº¤äº’æ¬¡æ•°ä»5æ”¹ä¸º2æˆ–3")
    print("7. å¢åŠ æ‰¹æ¬¡å¤§å°: å°è¯•2048æˆ–4096")

if __name__ == "__main__":
    main()