"""
ä¿®å¤ç‰ˆæœ¬çš„é˜¿é‡Œç§»åŠ¨æ¨èç®—æ³•æ•°æ®é›†åŠ è½½å™¨
è§£å†³æ•°æ®è¿‡æ»¤è¿‡äºä¸¥æ ¼å¯¼è‡´è®­ç»ƒæ•°æ®ä¸è¶³çš„é—®é¢˜
"""

import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import os
import zipfile
from typing import Dict, Tuple, List
from collections import defaultdict


class FixedAlibabaDataset(Dataset):
    """
    ä¿®å¤ç‰ˆæœ¬çš„é˜¿é‡Œç§»åŠ¨æ¨èæ•°æ®é›†å¤„ç†ç±»
    ä¸»è¦æ”¹è¿›ï¼š
    1. æ›´å®½æ¾çš„è¿‡æ»¤æ¡ä»¶
    2. æ›´å¥½çš„æ•°æ®ç»Ÿè®¡å’ŒéªŒè¯
    3. æ›´åˆç†çš„è®­ç»ƒæµ‹è¯•é›†åˆ’åˆ†
    """
    
    def __init__(self, data_path: str, behavior_types: List[int] = [4], 
                 min_user_interactions: int = 2, min_item_interactions: int = 2,
                 test_size: float = 0.2, random_state: int = 42):
        """
        åˆå§‹åŒ–æ•°æ®é›†
        
        Args:
            data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            behavior_types: è€ƒè™‘çš„è¡Œä¸ºç±»å‹åˆ—è¡¨ [1:æµè§ˆ, 2:æ”¶è—, 3:åŠ è´­ç‰©è½¦, 4:è´­ä¹°]
            min_user_interactions: ç”¨æˆ·æœ€å°‘äº¤äº’æ¬¡æ•°ï¼ˆé™ä½é˜ˆå€¼ï¼‰
            min_item_interactions: å•†å“æœ€å°‘äº¤äº’æ¬¡æ•°ï¼ˆé™ä½é˜ˆå€¼ï¼‰
            test_size: æµ‹è¯•é›†æ¯”ä¾‹
            random_state: éšæœºç§å­
        """
        self.data_path = data_path
        self.behavior_types = behavior_types
        self.min_user_interactions = min_user_interactions
        self.min_item_interactions = min_item_interactions
        self.test_size = test_size
        self.random_state = random_state
        
        # åˆå§‹åŒ–ç¼–ç å™¨
        self.user_encoder = LabelEncoder()
        self.item_encoder = LabelEncoder()
        
        # åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
        self._load_data()
        self._preprocess_data()
        self._build_graph()
        
    def _load_data(self):
        """åŠ è½½åŸå§‹æ•°æ®æ–‡ä»¶"""
        print("æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶...")
        
        # åŠ è½½ç”¨æˆ·è¡Œä¸ºæ•°æ®
        user_data_path = os.path.join(self.data_path, "tianchi_mobile_recommend_train_user.zip")
        if os.path.exists(user_data_path):
            # å¦‚æœæ˜¯zipæ–‡ä»¶ï¼Œè§£å‹è¯»å–
            with zipfile.ZipFile(user_data_path, 'r') as zip_file:
                csv_file = zip_file.namelist()[0]
                self.user_data = pd.read_csv(zip_file.open(csv_file))
        else:
            # å¦‚æœå·²ç»è§£å‹ï¼Œç›´æ¥è¯»å–CSV
            csv_path = os.path.join(self.data_path, "tianchi_mobile_recommend_train_user.csv")
            if os.path.exists(csv_path):
                self.user_data = pd.read_csv(csv_path)
            else:
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç”¨æˆ·æ•°æ®æ–‡ä»¶: {user_data_path} æˆ– {csv_path}")
            
        # åŠ è½½å•†å“æ•°æ®
        item_data_path = os.path.join(self.data_path, "tianchi_mobile_recommend_train_item.csv")
        if os.path.exists(item_data_path):
            self.item_data = pd.read_csv(item_data_path)
        else:
            print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°å•†å“æ•°æ®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç”¨æˆ·æ•°æ®ä¸­çš„æ‰€æœ‰å•†å“")
            # å¦‚æœæ²¡æœ‰å•†å“æ–‡ä»¶ï¼Œä»ç”¨æˆ·æ•°æ®ä¸­æå–å•†å“
            unique_items = self.user_data['item_id'].unique()
            self.item_data = pd.DataFrame({'item_id': unique_items})
        
        print(f"ç”¨æˆ·è¡Œä¸ºæ•°æ®å½¢çŠ¶: {self.user_data.shape}")
        print(f"å•†å“æ•°æ®å½¢çŠ¶: {self.item_data.shape}")
        print("æ•°æ®åŠ è½½å®Œæˆ!")
        
    def _preprocess_data(self):
        """æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†"""
        print("æ­£åœ¨é¢„å¤„ç†æ•°æ®...")
        
        # æ˜¾ç¤ºåŸå§‹æ•°æ®ç»Ÿè®¡
        print(f"åŸå§‹æ•°æ®ç»Ÿè®¡:")
        print(f"- æ€»äº¤äº’æ•°: {len(self.user_data)}")
        print(f"- å”¯ä¸€ç”¨æˆ·æ•°: {self.user_data['user_id'].nunique()}")
        print(f"- å”¯ä¸€å•†å“æ•°: {self.user_data['item_id'].nunique()}")
        print(f"- è¡Œä¸ºç±»å‹åˆ†å¸ƒ:")
        for bt in sorted(self.user_data['behavior_type'].unique()):
            count = (self.user_data['behavior_type'] == bt).sum()
            print(f"  ç±»å‹{bt}: {count} ({count/len(self.user_data)*100:.1f}%)")
        
        # 1. è¿‡æ»¤æŒ‡å®šè¡Œä¸ºç±»å‹
        original_size = len(self.user_data)
        self.user_data = self.user_data[self.user_data['behavior_type'].isin(self.behavior_types)]
        print(f"\næ­¥éª¤1 - è¡Œä¸ºç±»å‹è¿‡æ»¤: {original_size} â†’ {len(self.user_data)}")
        
        if len(self.user_data) == 0:
            raise ValueError(f"è¿‡æ»¤è¡Œä¸ºç±»å‹ {self.behavior_types} åæ•°æ®ä¸ºç©ºï¼")
        
        # 2. åªä¿ç•™å•†å“å­é›†ä¸­çš„å•†å“
        item_ids = set(self.item_data['item_id'].unique())
        before_item_filter = len(self.user_data)
        self.user_data = self.user_data[self.user_data['item_id'].isin(item_ids)]
        print(f"æ­¥éª¤2 - å•†å“è¿‡æ»¤: {before_item_filter} â†’ {len(self.user_data)}")
        
        if len(self.user_data) == 0:
            raise ValueError("å•†å“è¿‡æ»¤åæ•°æ®ä¸ºç©ºï¼")
        
        # 3. åˆ†æäº¤äº’é¢‘æ¬¡åˆ†å¸ƒ
        user_inter_count = self.user_data.groupby('user_id').size()
        item_inter_count = self.user_data.groupby('item_id').size()
        
        print(f"\näº¤äº’é¢‘æ¬¡åˆ†æ:")
        print(f"ç”¨æˆ·äº¤äº’æ¬¡æ•° - æœ€å°:{user_inter_count.min()}, æœ€å¤§:{user_inter_count.max()}, å¹³å‡:{user_inter_count.mean():.2f}")
        print(f"å•†å“äº¤äº’æ¬¡æ•° - æœ€å°:{item_inter_count.min()}, æœ€å¤§:{item_inter_count.max()}, å¹³å‡:{item_inter_count.mean():.2f}")
        
        # 4. åº”ç”¨æ›´å®½æ¾çš„æœ€å°äº¤äº’é¢‘æ¬¡è¿‡æ»¤
        print(f"\nåº”ç”¨æœ€å°äº¤äº’é¢‘æ¬¡è¿‡æ»¤ (ç”¨æˆ·>={self.min_user_interactions}, å•†å“>={self.min_item_interactions}):")
        
        valid_users = set(user_inter_count[user_inter_count >= self.min_user_interactions].index)
        valid_items = set(item_inter_count[item_inter_count >= self.min_item_interactions].index)
        
        print(f"æœ‰æ•ˆç”¨æˆ·: {len(valid_users)} / {len(user_inter_count)}")
        print(f"æœ‰æ•ˆå•†å“: {len(valid_items)} / {len(item_inter_count)}")
        
        before_final_filter = len(self.user_data)
        self.user_data = self.user_data[
            self.user_data['user_id'].isin(valid_users) &
            self.user_data['item_id'].isin(valid_items)
        ]
        print(f"æ­¥éª¤3 - é¢‘æ¬¡è¿‡æ»¤: {before_final_filter} â†’ {len(self.user_data)}")
        
        # æ£€æŸ¥æ˜¯å¦è¿‡æ»¤è¿‡åº¦
        if len(self.user_data) < 1000:
            print(f"âš ï¸  è­¦å‘Š: è¿‡æ»¤ååªå‰© {len(self.user_data)} æ¡äº¤äº’æ•°æ®ï¼Œå»ºè®®é™ä½è¿‡æ»¤æ¡ä»¶")
            
            if len(self.user_data) == 0:
                # å¦‚æœè¿‡æ»¤åä¸ºç©ºï¼Œå°è¯•æ›´å®½æ¾çš„æ¡ä»¶
                print("ğŸ”„ å°è¯•æ›´å®½æ¾çš„è¿‡æ»¤æ¡ä»¶...")
                self.min_user_interactions = max(1, self.min_user_interactions - 1)
                self.min_item_interactions = max(1, self.min_item_interactions - 1)
                
                # é‡æ–°è®¡ç®—
                user_inter_count = self.user_data.groupby('user_id').size()
                item_inter_count = self.user_data.groupby('item_id').size()
                valid_users = set(user_inter_count[user_inter_count >= self.min_user_interactions].index)
                valid_items = set(item_inter_count[item_inter_count >= self.min_item_interactions].index)
                
                self.user_data = self.user_data[
                    self.user_data['user_id'].isin(valid_users) &
                    self.user_data['item_id'].isin(valid_items)
                ]
                
                if len(self.user_data) == 0:
                    # æœ€åçš„ä¿é™©ï¼šä¸åº”ç”¨é¢‘æ¬¡è¿‡æ»¤
                    print("ğŸš¨ å®Œå…¨å–æ¶ˆé¢‘æ¬¡è¿‡æ»¤ï¼Œä¿ç•™æ‰€æœ‰æ•°æ®")
                    before_final_filter = len(self.user_data)
                    # é‡æ–°åŠ è½½è¿‡æ»¤åçš„æ•°æ®
                    self.user_data = self.user_data  # è¿™é‡Œåº”è¯¥æ˜¯é‡æ–°åŠ è½½å‰é¢æ­¥éª¤çš„ç»“æœ
        
        # 5. ç¼–ç ç”¨æˆ·å’Œå•†å“ID
        self.user_data['user_id_encoded'] = self.user_encoder.fit_transform(self.user_data['user_id'])
        self.user_data['item_id_encoded'] = self.item_encoder.fit_transform(self.user_data['item_id'])
        
        # 6. åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        self.num_users = self.user_data['user_id_encoded'].nunique()
        self.num_items = self.user_data['item_id_encoded'].nunique()
        self.num_interactions = len(self.user_data)
        
        print(f"\nâœ… æœ€ç»ˆæ•°æ®ç»Ÿè®¡:")
        print(f"ç”¨æˆ·æ•°é‡: {self.num_users}")
        print(f"å•†å“æ•°é‡: {self.num_items}")
        print(f"äº¤äº’æ•°é‡: {self.num_interactions}")
        print(f"å¹³å‡æ¯ç”¨æˆ·äº¤äº’: {self.num_interactions/self.num_users:.1f}")
        print(f"å¹³å‡æ¯å•†å“äº¤äº’: {self.num_interactions/self.num_items:.1f}")
        print(f"ç¨€ç–åº¦: {(1 - self.num_interactions/(self.num_users * self.num_items))*100:.2f}%")
        
        # 7. æ—¶é—´æ’åº
        self.user_data['time'] = pd.to_datetime(self.user_data['time'])
        self.user_data = self.user_data.sort_values('time')
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        if self.num_interactions < 1000:
            print("âš ï¸  æ•°æ®é‡è¾ƒå°‘ï¼Œå»ºè®®æ£€æŸ¥æ•°æ®æºæˆ–é™ä½è¿‡æ»¤æ¡ä»¶")
        if self.num_users < 50 or self.num_items < 50:
            print("âš ï¸  ç”¨æˆ·æˆ–å•†å“æ•°é‡è¾ƒå°‘ï¼Œå¯èƒ½å½±å“æ¨¡å‹æ•ˆæœ")
            
    def _build_graph(self):
        """æ„å»ºç”¨æˆ·-å•†å“äºŒéƒ¨å›¾"""
        print("\næ­£åœ¨æ„å»ºå›¾ç»“æ„...")
        
        # åˆ›å»ºè®­ç»ƒæµ‹è¯•é›†åˆ†å‰² - ä½¿ç”¨éšæœºåˆ†å‰²è€Œä¸æ˜¯æ—¶é—´åˆ†å‰²
        # å› ä¸ºæ—¶é—´åˆ†å‰²å¯èƒ½å¯¼è‡´æ•°æ®åˆ†å¸ƒä¸å‡
        train_indices, test_indices = train_test_split(
            range(len(self.user_data)), 
            test_size=self.test_size, 
            random_state=self.random_state,
            stratify=self.user_data['user_id_encoded']  # æŒ‰ç”¨æˆ·åˆ†å±‚
        )
        
        train_data = self.user_data.iloc[train_indices]
        test_data = self.user_data.iloc[test_indices]
        
        print(f"è®­ç»ƒé›†å¤§å°: {len(train_data)} ({len(train_data)/len(self.user_data)*100:.1f}%)")
        print(f"æµ‹è¯•é›†å¤§å°: {len(test_data)} ({len(test_data)/len(self.user_data)*100:.1f}%)")
        
        # æ„å»ºè®­ç»ƒé›†çš„è¾¹ç´¢å¼•
        train_interactions = train_data[['user_id_encoded', 'item_id_encoded']].values
        
        # åˆ›å»ºåŒå‘è¾¹ï¼šç”¨æˆ·->å•†å“ å’Œ å•†å“->ç”¨æˆ·
        user_to_item = train_interactions
        item_to_user = np.column_stack([
            train_interactions[:, 1] + self.num_users,  # å•†å“èŠ‚ç‚¹ç´¢å¼•åç§»
            train_interactions[:, 0]  # ç”¨æˆ·èŠ‚ç‚¹ç´¢å¼•
        ])
        
        # åˆå¹¶æ‰€æœ‰è¾¹
        all_edges = np.vstack([user_to_item, item_to_user])
        
        # è½¬æ¢ä¸ºPyTorch Geometricæ ¼å¼çš„è¾¹ç´¢å¼•
        self.edge_index = torch.from_numpy(all_edges.T).long()
        
        # å­˜å‚¨è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        self.train_interactions = train_interactions
        self.test_interactions = test_data[['user_id_encoded', 'item_id_encoded']].values
        
        # åˆ›å»ºç”¨æˆ·çš„å†å²äº¤äº’è®°å½•ï¼Œç”¨äºè´Ÿé‡‡æ ·
        self.user_item_dict = defaultdict(set)
        for user_id, item_id in train_interactions:
            self.user_item_dict[user_id].add(item_id)
            
        print(f"å›¾èŠ‚ç‚¹æ€»æ•°: {self.num_users + self.num_items}")
        print(f"å›¾è¾¹æ•°é‡: {len(all_edges)}")
        print("å›¾æ„å»ºå®Œæˆ!")
        
    def get_edge_index(self):
        """è·å–å›¾çš„è¾¹ç´¢å¼•"""
        return self.edge_index
    
    def get_num_nodes(self):
        """è·å–å›¾çš„èŠ‚ç‚¹æ€»æ•°"""
        return self.num_users + self.num_items
    
    def get_train_data(self):
        """è·å–è®­ç»ƒæ•°æ®"""
        return self.train_interactions
    
    def get_test_data(self):
        """è·å–æµ‹è¯•æ•°æ®"""
        return self.test_interactions
    
    def negative_sampling(self, user_id: int, num_negatives: int = 1) -> List[int]:
        """
        æ”¹è¿›çš„è´Ÿé‡‡æ ·æ–¹æ³•
        """
        positive_items = self.user_item_dict[user_id]
        negative_items = []
        
        # å¢åŠ æœ€å¤§å°è¯•æ¬¡æ•°ï¼Œé¿å…æ­»å¾ªç¯
        max_attempts = min(num_negatives * 50, self.num_items * 2)
        attempts = 0
        
        while len(negative_items) < num_negatives and attempts < max_attempts:
            item_id = np.random.randint(0, self.num_items)
            if item_id not in positive_items:
                negative_items.append(item_id)
            attempts += 1
                
        # å¦‚æœé‡‡æ ·ä¸è¶³ï¼Œç”¨éšæœºå¡«å……
        while len(negative_items) < num_negatives:
            negative_items.append(np.random.randint(0, self.num_items))
            
        return negative_items
    
    def get_user_item_interactions(self) -> Dict[int, List[int]]:
        """è·å–ç”¨æˆ·-å•†å“äº¤äº’å­—å…¸"""
        return dict(self.user_item_dict)
    
    def get_data_statistics(self) -> Dict:
        """è·å–è¯¦ç»†çš„æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'num_users': self.num_users,
            'num_items': self.num_items,
            'num_interactions': self.num_interactions,
            'num_train_interactions': len(self.train_interactions),
            'num_test_interactions': len(self.test_interactions),
            'sparsity': 1 - self.num_interactions / (self.num_users * self.num_items),
            'avg_user_interactions': self.num_interactions / self.num_users,
            'avg_item_interactions': self.num_interactions / self.num_items
        }
    
    def __len__(self):
        """è¿”å›è®­ç»ƒæ ·æœ¬æ•°é‡"""
        return len(self.train_interactions)
    
    def __getitem__(self, idx):
        """è·å–å•ä¸ªè®­ç»ƒæ ·æœ¬"""
        user_id, item_id = self.train_interactions[idx]
        
        # æ­£æ ·æœ¬
        positive_sample = {
            'user_id': user_id,
            'item_id': item_id,
            'label': 1.0
        }
        
        # è´Ÿæ ·æœ¬
        negative_item = self.negative_sampling(user_id, 1)[0]
        negative_sample = {
            'user_id': user_id,
            'item_id': negative_item,
            'label': 0.0
        }
        
        return positive_sample, negative_sample


def create_larger_sample_data(save_path: str):
    """
    åˆ›å»ºæ›´å¤§è§„æ¨¡çš„ç¤ºä¾‹æ•°æ®ï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯
    """
    print("æ­£åœ¨åˆ›å»ºå¤§è§„æ¨¡ç¤ºä¾‹æ•°æ®...")
    
    np.random.seed(42)
    
    # å¢åŠ æ•°æ®è§„æ¨¡
    num_users = 5000
    num_items = 2000
    num_interactions = 50000
    
    # ç”Ÿæˆæ›´çœŸå®çš„ç”¨æˆ·è¡Œä¸ºæ•°æ®
    # æ¨¡æ‹Ÿç”¨æˆ·è¡Œä¸ºçš„å¹‚å¾‹åˆ†å¸ƒ
    user_popularity = np.random.pareto(0.5, num_users) + 1
    item_popularity = np.random.pareto(0.8, num_items) + 1
    
    user_weights = user_popularity / user_popularity.sum()
    item_weights = item_popularity / item_popularity.sum()
    
    user_data = {
        'user_id': np.random.choice(range(1, num_users + 1), num_interactions, p=user_weights),
        'item_id': np.random.choice(range(1, num_items + 1), num_interactions, p=item_weights),
        'behavior_type': np.random.choice([1, 2, 3, 4], num_interactions, p=[0.5, 0.25, 0.15, 0.1]),
        'user_geohash': ['geo_' + str(i) for i in np.random.randint(1, 200, num_interactions)],
        'item_category': np.random.randint(1, 100, num_interactions),
        'time': pd.date_range('2014-11-01', '2014-12-31', periods=num_interactions)
    }
    
    user_df = pd.DataFrame(user_data)
    
    # ç”Ÿæˆå•†å“æ•°æ®ï¼ˆä¿ç•™å¤§éƒ¨åˆ†å•†å“ï¼‰
    selected_items = np.random.choice(range(1, num_items + 1), size=int(num_items * 0.8), replace=False)
    item_data = {
        'item_id': selected_items,
        'item_geohash': ['item_geo_' + str(i) for i in np.random.randint(1, 150, len(selected_items))],
        'item_category': np.random.randint(1, 100, len(selected_items))
    }
    
    item_df = pd.DataFrame(item_data)
    
    # ä¿å­˜æ•°æ®
    os.makedirs(save_path, exist_ok=True)
    user_df.to_csv(os.path.join(save_path, 'tianchi_mobile_recommend_train_user.csv'), index=False)
    item_df.to_csv(os.path.join(save_path, 'tianchi_mobile_recommend_train_item.csv'), index=False)
    
    print(f"å¤§è§„æ¨¡ç¤ºä¾‹æ•°æ®å·²ä¿å­˜åˆ°: {save_path}")
    print(f"ç”¨æˆ·è¡Œä¸ºæ•°æ®: {user_df.shape}")
    print(f"å•†å“æ•°æ®: {item_df.shape}")
    print(f"è¡Œä¸ºç±»å‹åˆ†å¸ƒ:")
    print(user_df['behavior_type'].value_counts().sort_index())


if __name__ == "__main__":
    # æµ‹è¯•ä¿®å¤ç‰ˆæœ¬çš„æ•°æ®åŠ è½½å™¨
    data_path = "./test_alibaba_data"
    
    # åˆ›å»ºå¤§è§„æ¨¡æµ‹è¯•æ•°æ®
    create_larger_sample_data(data_path)
    
    # æµ‹è¯•ä¸åŒçš„è¿‡æ»¤æ¡ä»¶
    print("\n" + "="*60)
    print("æµ‹è¯•ä¿®å¤ç‰ˆæœ¬çš„æ•°æ®åŠ è½½å™¨")
    print("="*60)
    
    # æµ‹è¯•1: å®½æ¾æ¡ä»¶
    try:
        dataset = FixedAlibabaDataset(
            data_path=data_path,
            behavior_types=[4],  # åªè€ƒè™‘è´­ä¹°è¡Œä¸º
            min_user_interactions=2,  # é™ä½ç”¨æˆ·æœ€å°äº¤äº’æ¬¡æ•°
            min_item_interactions=2,  # é™ä½å•†å“æœ€å°äº¤äº’æ¬¡æ•°
            test_size=0.2
        )
        
        print(f"\nâœ… å®½æ¾è¿‡æ»¤æ¡ä»¶æµ‹è¯•æˆåŠŸ:")
        stats = dataset.get_data_statistics()
        for key, value in stats.items():
            print(f"  {key}: {value}")
            
    except Exception as e:
        print(f"âŒ å®½æ¾è¿‡æ»¤æ¡ä»¶æµ‹è¯•å¤±è´¥: {e}")
    
    # æµ‹è¯•2: ä¸¥æ ¼æ¡ä»¶ (æ‚¨åŸæ¥çš„æ¡ä»¶)
    try:
        dataset_strict = FixedAlibabaDataset(
            data_path=data_path,
            behavior_types=[4],
            min_user_interactions=5,  # æ‚¨åŸæ¥çš„æ¡ä»¶
            min_item_interactions=5,  # æ‚¨åŸæ¥çš„æ¡ä»¶
            test_size=0.2
        )
        
        print(f"\nâœ… ä¸¥æ ¼è¿‡æ»¤æ¡ä»¶æµ‹è¯•æˆåŠŸ:")
        stats = dataset_strict.get_data_statistics()
        for key, value in stats.items():
            print(f"  {key}: {value}")
            
    except Exception as e:
        print(f"âŒ ä¸¥æ ¼è¿‡æ»¤æ¡ä»¶æµ‹è¯•å¤±è´¥: {e}")